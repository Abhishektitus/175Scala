1. copy local to hdfs:
	[paslechoix@gw03 ~]$ hdfs dfs -put mgr1.csv spark1/
	[paslechoix@gw03 ~]$ hdfs dfs -ls spark1
	-rw-r--r--   3 paslechoix hdfs         40 2018-02-26 21:17 spark1/mgr1.csv

2. copy hdfs to local:
	[paslechoix@gw03 data]$ hdfs dfs -ls spark2/sorted_peter.txt/ *
	-rw-r--r--   3 paslechoix hdfs          0 2018-02-26 21:59 spark2/sorted_peter.txt/_SUCCESS
	-rw-r--r--   3 paslechoix hdfs       3194 2018-02-26 21:59 spark2/sorted_peter.txt/part-00000
	-rw-r--r--   3 paslechoix hdfs       9882 2018-02-26 21:59 spark2/sorted_peter.txt/part-00001
	[paslechoix@gw03 data]$ hdfs dfs -copyToLocal spark2/sorted_peter.txt/part-*

	[paslechoix@gw03 data]$ ls part*
	part-00000  part-00001

3. Concatenate text files in linux:
	[paslechoix@gw03 data]$ cat part-00000 part-00001 > sorted.txt


4. Create external table on hive based on existing file on hdfs

4.1 Create a new text file.
	vim file_on_local.csv

4.2 Create a new directory on HDFS.
	hdfs dfs -mkdir /apps/hive/warehouse/paslechoix.db/file_on_local

4.3 Place the file on HDFS.
	hdfs dfs -put file_on_local.csv /apps/hive/warehouse/paslechoix.db/file_on_local/

4.4 Execute below command on Hive terminal.
	
	hive (default)> 
	create external table file_on_hive_tbl(id int, name string) 
	row format delimited fields terminated by ',' 
	stored as textfile 
	location '/apps/hive/warehouse/paslechoix.db/file_on_local/';

4.5 Check the table on hive
	hive (default)> select * from file_on_hive_tbl;
	OK
	2       Fitness
	3       Footwear
	4       Apparel
	5       Golf
	6       Outdoors
	7       Fan Shop
	Time taken: 0.46 seconds, Fetched: 6 row(s)



5. sqoop import:
6. import from mysql to hdfs:
7. import from mysql to hive:


	sqoop import \
	--connect jdbc:mysql://ms.itversity.com/retail_export \
	--username=retail_user \
	--password=itversity \
	--table=departments \
	--target-dir=department_hive3 \
	--hive-import \
	--hive-table=paslechoix1.department_hive3 \
	--incremental append \
	--check-column department_id \
	-m 1

sqoop export:
export from hdfs to mysql:
export from hive to mysql:
	
	export the hive table departments to mysql:

	hive (paslechoix)> select * from departments;
	OK
	2       Fitness
	3       Footwear
	4       Apparel
	5       Golf
	6       Outdoors
	7       Fan Shop

	hive (paslechoix)> show create table departments;
	OK
	CREATE TABLE `departments`(
	  `department_id` int,
	  `department_name` string)
	COMMENT 'Imported by sqoop on 2018/01/31 14:53:33'
	ROW FORMAT DELIMITED
	  FIELDS TERMINATED BY '\u0001'
	  LINES TERMINATED BY '\n'
	STORED AS INPUTFORMAT
	  'org.apache.hadoop.mapred.TextInputFormat'
	OUTPUTFORMAT
	  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
	LOCATION
	  'hdfs://nn01.itversity.com:8020/apps/hive/warehouse/paslechoix.db/departments'
	TBLPROPERTIES (
	  'numFiles'='3',
	  'numRows'='0',
	  'rawDataSize'='0',
	  'totalSize'='90',
	  'transient_lastDdlTime'='1517428412')
	Time taken: 0.134 seconds, Fetched: 19 row(s)


	mysql> create table departments_from_hive (id int, name varchar(20));


	sqoop export \
	--connect jdbc:mysql://ms.itversity.com/retail_export \
	--username=retail_user \
	--password=itversity \
	--table departments_from_hive \
	--export-dir /apps/hive/warehouse/paslechoix.db/departments

	mysql> select * from departments_from_hive;

	nothing exported!
	Reason: format is wrong! in hive directory: data in snappy compressed format
	[paslechoix@gw03 ~]$ hdfs dfs -ls /apps/hive/warehouse/paslechoix.db/departments
	Found 3 items
	-rwxrwxrwx   3 paslechoix hdfs         31 2018-01-31 14:53 /apps/hive/warehouse/paslechoix.db/departments/part-m-00000.snappy
	-rwxrwxrwx   3 paslechoix hdfs         27 2018-01-31 14:53 /apps/hive/warehouse/paslechoix.db/departments/part-m-00001.snappy
	-rwxrwxrwx   3 paslechoix hdfs         32 2018-01-31 14:53 /apps/hive/warehouse/paslechoix.db/departments/part-m-00002.snappy

	[paslechoix@gw03 ~]$ hdfs dfs -ls /apps/hive/warehouse/paslechoix.db/departments_hive01                                                   Found 2 items
	-rwxrwxrwx   3 paslechoix hdfs          0 2018-02-04 16:34 /apps/hive/warehouse/paslechoix.db/departments_hive01/_SUCCESS
	-rwxrwxrwx   3 paslechoix hdfs         72 2018-02-04 16:34 /apps/hive/warehouse/paslechoix.db/departments_hive01/part-m-00000

	[paslechoix@gw03 ~]$ hdfs dfs -cat /apps/hive/warehouse/paslechoix.db/departments_hive01/part-m-00000
	2Fitness0
	3Footwear0
	4Apparel0
	5Golf0
	6Outdoors0
	7Fan Shop0
	[paslechoix@gw03 ~]$

	sqoop export \
	--connect jdbc:mysql://ms.itversity.com/retail_export \
	--username=retail_user \
	--password=itversity \
	--table departments_from_hive \
	--export-dir /apps/hive/warehouse/paslechoix.db/departments_hive01

	mysql> select * from departments_from_hive;


map example:
	val mgrMap1 = mgr1.map(x => (x.split(",")(0).toInt, x.split(",")(1).trim))

	scala> joined
	res14: org.apache.spark.rdd.RDD[(Int, (String, Int))] = MapPartitionsRDD[8] at join at <console>:35

	val joinedMap1 = joined1.map(x=>(x._1, x._2._1, x._2._2))

flatMap example:



sortBy example:

	val sorted = joined1.sortByKey() 
	val sorted1 = joined1.sortBy(_._2._1)

save RDD to hdfs:

	scala> finalData.saveAsTextFile("spark1/result.csv") 
	[paslechoix@gw03 ~]$ hdfs dfs -cat spark1/result.csv/*

